% -*- coding-system:utf-8 
% LATEX PREAMBLE --- needs to be imported manually
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\pagestyle{empty}
\setlength{\parindent}{0in}

%%% commands that do not need to imported into Anki:
\usepackage{mdframed}
\newcommand*{\xfield}[1]{\begin{mdframed}\centering #1\end{mdframed}\bigskip}
\newenvironment{note}{}{}
% END OF THE PREAMBLE

\begin{document}

\begin{note}
    \xfield{How do we qualify a system of equation with one or more solution ?}
    \xfield{consistent}
\end{note}

\begin{note}
    \xfield{How do we qualify a system of equation with no solution ?}
    \xfield{inconsistent}
\end{note}

\begin{note}
    \xfield{What does it mean if two systems of equations are equivalent ?}
    \xfield{They have the same solutions sets}
\end{note}

\begin{note}
    \xfield{What are the possible elementary row operations ?}
    \xfield{\begin{itemize} \item Swap arrows \item  multiply a row by a non zero constant \item  add a row to another \end{itemize}}
\end{note}

\begin{note}
    \xfield{Once a system of equation has been transformed in an augmented matrix and put in echelon form through elementary row operations, what are the different possibilities ?}
    \xfield{\begin{itemize} \item if last row (0 0 0 0 $\neq$ | 0), then inconsistent \item if \#pivots=\#variable, then 1 solution \item if \#pivots $<$ \#variables, then $\infty$ solutions \end{itemize}}
\end{note}

\begin{note}
    \xfield{What is the definition for a set of vectors to be linearly dependent}
    \xfield{It means that when the linear combination of the set is equal to the vector zero (i.e \begin{math}c_1\underline{v_1}+c_2\underline{v_2}+...+c_n\underline{v_n}=\underline{0}\end{math}), it has a non-zero solution}
\end{note}

\begin{note}
    \xfield{What is the definition for a set of vectors to be linearly independent}
    \xfield{It means that when the linear combination of the set is equal to the vector zero (i.e \begin{math}c_1\underline{v_1}+c_2\underline{v_2}+...+c_n\underline{v_n}=\underline{0}\end{math}), it has no non-zero solution (and the vector zero is the only solution)}
\end{note}

\begin{note}
    \xfield{Define the rank of a matrix}
    \xfield{It is the maximal number r, s.t. A has r lin. indep. columns (in echelon form, it can be deduced from the numbers of pivots)}
\end{note}

\begin{note}
    \xfield{for A and B two matrices, \begin{math}(AB)^T\ =\ ?\end{math}}
    \xfield{\begin{math}B^TA^T\end{math}}
\end{note}

\begin{note}
    \xfield{Define the condition for a matrix \begin{math}A \in \mathbb{R}^{n\cdot n} \end{math} to be invertible}
    \xfield{Invertible if there exists a \begin{math}C \in \mathbb{R}^{n\cdot n}\ s.t.\ CA = I\ \text{and}\ AC = I\end{math}\\
    The rank($A$) also needs to be equal to n}
\end{note}

\begin{note}
    \xfield{Give the definition of being onto (surjective) for a transformation $T : X \rightarrow Y$ and how it translates when the matrix is in echelon form}
    \xfield{T is onto if for every \begin{math}y \in Y\end{math} there is \underline{at least one} \begin{math}x \in X \text{s.t.} T(x)=y\end{math} in echelon form, no (0...0 0 0) row}
\end{note}

\begin{note}
    \xfield{Give the definition of being bijective for a transformation $T : X \rightarrow Y$ and how it translates when the matrix is in echelon form}
    \xfield{T is onto if for every \begin{math}y \in Y\end{math} there is \underline{exactly one} \begin{math}x \in X \text{s.t.} T(x)=y\end{math} in echelon form, no (0... 0 0 0 ) row and \#pivots = \#vars}
\end{note}

\begin{note}
    \xfield{Give the definition of being one-to-one (injective) for a transformation $T : X \rightarrow Y$ and how it translates when the matrix is in echelon form}
    \xfield{T is one-to-one if for every \begin{math}y \in Y\end{math} there is \underline{at most one} \begin{math}x \in X \text{s.t.} T(x)=y\end{math} In echelon form, it means that \#pivots = \#vars}
\end{note}

\begin{note}
\xfield{How to find the inverse of the matrix $A$ using row reduction ?}
\xfield{Row reduce $(A\lvert I) \rightarrow \cdots \rightarrow (I \lvert A^{-1})$}
\end{note}

\begin{note}
\xfield{How to invert the following matrix  $ A = \left(
    \begin{array}{c|c}
      A_{11}&A_{12}\\\cline{1-2}
      0&A_{22}
    \end{array}
  \right)$}
\xfield{$A^{-1} \left(
    \begin{array}{c|c}
      A^{-1}_{11}&-A^{-1}_{11}A_{12}A^{-1}_{22}\\\cline{1-2}
      0&A^{-1}_{22}
    \end{array}
  \right)$}
\end{note}

\begin{note}
\xfield{How to $LU$ factorize a matrix}
\xfield{Let's show an example : $A =  \left(
    \begin{array}{cccc}
      2&4&-1&-2\\
      -4&-5&3&1\\
      2&-5&-3&8
    \end{array}
  \right)$ \\
  Reduce A to echelon form U (i.e. using only the follwing : adding a multiple of a row to a row below.)\\
$\left(
    \begin{array}{cccc}
      \cline{1-1}
      \multicolumn{1}{|c}2&\multicolumn{1}{|c}4&-1&-2\\
      \multicolumn{1}{|c}-4&\multicolumn{1}{|c}-5&3&1\\
      \multicolumn{1}{|c}2&\multicolumn{1}{|c}-5&-3&8\\
      \cline{1-1}
    \end{array}
  \right) \sim \left(
    \begin{array}{cccc}
      2&4&-1&-2\\
      \cline{2-2}
      0&\multicolumn{1}{|c}3&\multicolumn{1}{|c}1&-3\\
      0&\multicolumn{1}{|c}-9&\multicolumn{1}{|c}-2&10\\
      \cline{2-2}
    \end{array}
  \right) \sim \left(
    \begin{array}{cccc}
      2&4&-1&-2\\
      0&3&1&-3\\
      \cline{3-3}
      0&0&\multicolumn{1}{|c}1&\multicolumn{1}{|c}1\\
      \cline{3-3}
    \end{array} 
  \right) = U$ \\
  Then to find L your need to "normalize" the squared values : $\begin{matrix}
  \cline{1-1}
\multicolumn{1}{|c|}2 &\\
\cline{2-2}
\multicolumn{1}{|c|}{-4} & \multicolumn{1}{|c|}3 \\ 
\cline{3-3}
\multicolumn{1}{|c|}2 &\multicolumn{1}{|c|} {-9} &\multicolumn{1}{|c|} 1\\
\cline{1-3}\\
/2 & /3
\end{matrix}\ \ \rightarrow L = \left(
    \begin{array}{ccc}
      1&0&0\\
      -2&1&0\\
      1&-3&1
    \end{array}
  \right)$ }
\end{note}

\begin{note}
\xfield{Define what it means that $V$ is a subspace of $W$}
\xfield{$V$ has to satisfy the three following properties :\\
\begin{enumerate}
\item $0 \in V$
\item $\forall x,y \in V$ $\rightarrow$ $x + y \in V$
\item $\forall x \in V$ $\rightarrow$ $c \in \mathbb{R}$, $c \cdot x \in V$
\end{enumerate} }
\end{note}

\begin{note}
\xfield{Give the definition the column space of $A$ and how to find a basis for it.}
\xfield{The column space of $A$ denoted by Col($A$) is the set of all linear combination of the columns\\
to find a basis, your have to row reduce $(A|0)$ the columns with pivots forms the basis}
\end{note}

\begin{note}
\xfield{Give the definition the column space of $A$ and how to find a basis for it.}
\xfield{The nulspace of $A$ denoted by Nul($A$) is the set of all solutions of the equation $Ax=0$\\
You can find it by row reducing $(A|0)$ and treat it like a parametric equation. If there are free variables, it will give you a basis.}
\end{note}

\begin{note}
\xfield{Howto you compute the determinant of the matrix $A \in \mathbb{R}^{nxn}$}
\xfield{To find det($A$) = $\begin{vmatrix}
 	  a_{11}&\cdots&a_{1j}&\cdots&a_{1n}\\
      \vdots&\ddots&\vdots&\ddots&\vdots\\
      a_{i1}&\cdots&a_{ij}&\cdots&a_{in}\\
      \vdots&\ddots&\vdots&\ddots&\vdots\\
      a_{n1}&\cdots&a_{nj}&\cdots&a_{nn}\\
\end{vmatrix}$ \\First let's give the following notation : $A_{ij}$ is the matrix obtained from $A$ after having removed the $i$th row and the $j$th column.\\
Then, we know that det$A = a_{11}$det$A_{11} - a_{12}$det$A_{12} +a_{13}$det$A_{13} - \cdots + (-1)^{n+1} a_{1n}$det$A_{1n}$\\
From that and from the fact that $\begin{vmatrix}
a&b\\
c&d
\end{vmatrix} = ad-bc$ we can recursively find the determinant of the matrix A. 4}
\end{note}

\begin{note}
	\xfield{Give the effects of the elementary row operations on the determinant of the matrix $A = [a]_n \in \mathbb{R}^{n\times n}$\\
	Explain how you can use this to compute det($A$).}
	\xfield{$(1): \quad$ Applying $r_i \to ar_i$ has the effect of multiplying $\det \left({\mathbf A}\right)$ by $a$.\\
$(2): \quad$ Applying $r_i \to r_i + ar_j$ has no effect on $\det \left({\mathbf A}\right)$.\\
$(3): \quad$ Applying $r_i \leftrightarrow r_j$ has the effect of multiplying $\det \left({\mathbf A}\right)$ by $-1$.\\
Let $U$ be the echelon form of the matrix $A$ obtained without the use of row multiplication by a constant, then \\
det($A) = (-1)^r \text{det}(U)$ where $r$ is the number of row exchange used.}
\end{note}

\begin{note}
	\xfield{Give the result of the following equalitites :\\
	\begin{enumerate}
	\item if det$(A) \neq 0$, then ?
	\item det$(I_n) =$ ? where $I_n$ is the $n \times n$ identity matrix.
	\item det$(A^T) =$ ?
	\item det$(A^-1) =$ ?
	\item For square matrix $A$ and $B$ of equal size\\
	det$(AB) =$ ?
	\item det$(cA) =$ ? for a $n \times n$ matrix
	\item If $A$ is a triangular matrix, what is the determinant ?
	\end{enumerate} }
	\xfield{\begin{enumerate}
	\item Then $A^{-1} exists$
	\item det$(I_n) = 1$ where $I_n$ is the $n \times n$ identity matrix.
	\item det$(A^T) =$ det$(A)$
	\item det$(A^-1) = \frac{1}{\text{det}(A)} = \text{det}(A)^{-1}$
	\item For square matrix $A$ and $B$ of equal size\\
	det$(AB) = \text{det}(A)\text{det}(B)$
	\item det$(cA) = c^n$ det$(A)$ for a $n \times n$ matrix
	\item If $A$ is a triangular matrix, the determinant is equal to the sum of the product of the diagonal entries.
	\end{enumerate} }
\end{note}

\begin{note}
	\xfield{Consider a system of $n$ linear equations for $n$ unknowns, represented in matrix multiplication form as follows:\\
	$Ax = b$\\
	Solve it using the cramer's rule.}
	\xfield{Consider a system of $n$ linear equations for $n$ unknowns, represented in matrix multiplication form as follows:\\
	$Ax = b$\\
where the $n$ by $n$ matrix $ A $ has a nonzero determinant, and the vector $ x = (x_1, \ldots, x_n)^\mathrm{T} $ is the column vector of the variables.\\
Then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by:\\
:$ x_i = \frac{\det(A_i)}{\det(A)} \qquad i = 1, \ldots, n \, $\\
where $ A_i $ is the matrix formed by replacing the ''i''th column of $ A $ by the column vector $ b $.}
\end{note}

\begin{note}
	\xfield{Consider the $3\times 3$ matrix\\
$\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix}$\\
How to compute its adjugate ?\\
How can you use it to compute $A^{-1}$ ?}
	\xfield{Consider the $3\times 3$ matrix :
$\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix}$\\
Its adjugate is the transpose of the cofactor matrix\\
$\mathbf{C} = \begin{pmatrix}
+\left| \begin{matrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{matrix} \right| &
-\left| \begin{matrix} a_{21} & a_{23} \\ a_{31} & a_{33}  \end{matrix} \right| &
+\left| \begin{matrix} a_{21} & a_{22} \\ a_{31} & a_{32}  \end{matrix} \right| \\
 & & \\
-\left| \begin{matrix} a_{12} & a_{13} \\ a_{32} & a_{33} \end{matrix} \right| &
+\left| \begin{matrix} a_{11} & a_{13} \\ a_{31} & a_{33} \end{matrix} \right| &
-\left| \begin{matrix} a_{11} & a_{12} \\ a_{31} & a_{32} \end{matrix} \right| \\
 & & \\
+\left| \begin{matrix} a_{12} & a_{13} \\ a_{22} & a_{23} \end{matrix} \right| &
-\left| \begin{matrix} a_{11} & a_{13} \\ a_{21} & a_{23} \end{matrix} \right| &
+\left| \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix} \right|
\end{pmatrix}$\\
$\mathbf{C}^T = \operatorname{adj}(\mathbf{A}) = \begin{pmatrix}
+\left| \begin{matrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{matrix} \right| &
-\left| \begin{matrix} a_{12} & a_{13} \\ a_{32} & a_{33}  \end{matrix} \right| &
+\left| \begin{matrix} a_{12} & a_{13} \\ a_{22} & a_{23} \end{matrix} \right| \\
 & & \\
-\left| \begin{matrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{matrix} \right| &
+\left| \begin{matrix} a_{11} & a_{13} \\ a_{31} & a_{33} \end{matrix} \right| &
-\left| \begin{matrix} a_{11} & a_{13} \\ a_{21} & a_{23}  \end{matrix} \right| \\
 & & \\
+\left| \begin{matrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{matrix} \right| &
-\left| \begin{matrix} a_{11} & a_{12} \\ a_{31} & a_{32} \end{matrix} \right| &
+\left| \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix} \right|
\end{pmatrix}$\\
where\\
$\left| \begin{matrix} a_{im} & a_{in} \\ \,\,a_{jm} & a_{jn} \end{matrix} \right|=
\det\left(    \begin{matrix} a_{im} & a_{in} \\ \,\,a_{jm} & a_{jn} \end{matrix} \right)$.\\
then\\
$\frac{1}{\text{det}(\mathbf{A})}\operatorname{adj}(\mathbf{A})= A^{-1}$ 
}
\end{note}

\begin{note}
	\xfield{How to compute :
	\begin{itemize}
		\item dim(Col($A$))
		\item dim(Row($A$))
		\item dim(Nul($A$))
	\end{itemize}
	of $A \in \mathbb{R}^{n\times n}$ }
	\xfield{\begin{itemize}
		\item dim(Col($A$)) = rank($A$)
		\item dim(Row($A$)) = rank($A^T$)
		\item dim(Nul($A$)) = n - rank($A$)
	\end{itemize}}
\end{note}

\begin{note}
	\xfield{What are the conditions for $\mathcal{B} = \{ \underline{b_1},\underline{b_2},...,\underline{b_n}\}$ to be a basis of $V$ ?}
	\xfield{\begin{itemize}
	\item Span$(\underline{b_1},\underline{b_2},...,\underline{b_n}) = V$ s.t. any $v \in V$ can be written as linear combination of $\underline{b_i}$
	\item $\mathcal{B}$ is linearly independent.
	\end{itemize} }
\end{note}

\begin{note}
	\xfield{How to convert $V$ to $[V]_\mathcal{B}$ ?}
	\xfield{Row reduce $(B|V) \rightarrow (I|[V]_\mathcal{B}$}
\end{note}

\begin{note}
	\xfield{How to find the matrix of base tranformation $P_{\mathcal{B}\rightarrow\mathcal{C}}$ given $\mathcal{B} = \{ \underline{b_1},\underline{b_2},...,\underline{b_n}\}$ and $\mathcal{C} = \{ \underline{c_1},\underline{c_2},...,\underline{c_n}\}$ ?}
	\xfield{Row reduce $(C|B) \rightarrow (I|P_{\mathcal{B}\rightarrow\mathcal{C}})$}
\end{note}

\begin{note}
	\xfield{How to find the transformation matrix of $T$ with a change of base $[T]_{\mathcal{B}\rightarrow\mathcal{C}}$ given $T$ $\mathcal{B} = \{ \underline{b_1},\underline{b_2},...,\underline{b_n}\}$ and $\mathcal{C} = \{ \underline{c_1},\underline{c_2},...,\underline{c_n}\}$ ?}
	\xfield{First, you need to find $A$, s.t. $T(x) = Ax$, then row reduce $(C|AB) \rightarrow (I|\ [T]_{\mathcal{B}\rightarrow\mathcal{C}})$ }
\end{note}

\begin{note}
	\xfield{For the matrix $A \in \mathbb{R}^{n\times n}$\\
	 \begin{itemize}
	 	 \item Define what an eigenvalue is.
	\item Define what an eigenvector is.
	\item Give some relationship/interesting facts for eignevalues and eigenvectors.
	 \end{itemize} }
	\xfield{If $A\vec{v} = \lambda \vec{v}$, with non-zero $\vec{v} \in \mathbb{R}^n, \lambda \in \mathbb{R}$ \\
	Then $\lambda$ is eigenvalue of $A$\\
	and $\vec{v}$ is eigenvector of $A$ (for $\lambda$) \\
	\begin{itemize}
		\item $0$ can be eigenvalue, but $\vec{0}$ cannot be an eigenvector.
		\item An eigenvector corresponds to exactly one eigenvalue.
		\item An eigenvalue has many different values :
			\subitem $A\vec{v} = \lambda \vec{v}$  then $c\cdot \vec{v}, with c \in \mathbb{R}$ is also an eigenvector
			\subitem An eigenvalue can have many linear independent : \\
			e.g. $A = \begin{pmatrix}
			2&0\\
			0&2
			\end{pmatrix} \rightarrow \lambda = 2$ with $\vec{v_1}=\begin{pmatrix}
			1\\
			0
			\end{pmatrix},\vec{v_2} = \begin{pmatrix}
			0\\
			1
			\end{pmatrix} \rightarrow \text{any } \vec{u} \in \text{Span}\{\vec{v_1},\vec{v_2} \}$ is an eigenvector.
		\item Different eigenvalues have linearly independent eigenvectors. :\\
		$\lambda_1 \neq \lambda_2, A\vec{v_1} = \lambda_1 \vec{v_1}, A\vec{v_2} = \lambda_2 \vec{v_2} \rightarrow \vec{v_1},\vec{v_2}$ are linearly independent.
		\item For $A \in \mathbb{R}^{n\times n} \rightarrow$ \# different egeinvalues is $\le n$
		\item Some matrices don't have any eigenvalues in $\mathbb{R}$, for example $\begin{pmatrix}
		0&1\\
		-1&0
		\end{pmatrix}$ however they have eigenvalues in $\mathbb{C}$ i.e. complex eigenvalues (in our case $i$ and $-i$)
	\end{itemize} }  
\end{note}

\begin{note}
	\xfield{How to find the eigenvalues and eigenvectors (or eigenspace) of the matrix $A$ ?}
	\xfield{\begin{itemize}
		\item To find the eigenvalues, resolve det($A - \lambda I) = 0$
		\item To find the eigenspace row reduce $(A-\lambda I | 0) \rightarrow (REF|0) \rightarrow$ solution of the equation = basis
	\end{itemize} }
\end{note}

\begin{note}
	\xfield{What is the condition for $A$ and $B$, $n \times n $ matrices to be similar ?\\
	 How to find easily if they are similar ?}
	\xfield{If $A$ and $B$ are similar, then $B=P^{-1} A P$ for some invertible $n \times n$ matrix $P$ .\\
	 If $A$ and $B$ are similar, then they have the same eigenvalues. }
\end{note}

\begin{note}
	\xfield{\begin{enumerate}
	\item Define what it means for $A$, an $n\times n$ matrix to be "diagonalizable" ?
	\item Define what is the condition for a matrix to be "diagonalizable" ?
	\item Explain how to "diagonalizable" a matrix.
	\item Explain how it can help you to compute $A^k$
	\end{enumerate} }
	\xfield{\begin{enumerate}
	\item $A$ is "diagonalizable" if there exists the diagonal matrix $D$, s.t. $A=PDP^{-1}$\\
	\item A $n \times n$ matrix A is "diagonalizable" if and only if A has n linearly independent eigenvectors.
	\item To diagonalize $A$ a $n \times n$ matrix, you first have to find the eigenvalues of $A$ and then, you need to find a particular eigenvector for each eigenvalue, then $P=(v_1\ v_2\ ...\ v_n)$ and $D = \begin{pmatrix}
	\lambda_1 & 0 & \cdots & 0\\
	0 & \lambda_2 & \cdots & 0\\
	\vdots&\vdots&\ddots&\vdots\\
	0&0&\cdots&\lambda_n
	\end{pmatrix}$\\
	You can then check your result by checking if $PDP^{-1} = A$
	\item $A^k = P D^k P^{-1}$ and $D^k$ is trivial to compute. ($D^k = \begin{pmatrix}
	d_1^k & 0 & \cdots & 0\\
	0 & d_2^k& \cdots & 0\\
	\vdots&\vdots&\ddots&\vdots\\
	0&0&\cdots&d_n^k
	\end{pmatrix}$)
	\end{enumerate} }
\end{note}

\begin{note}
	\xfield{Let $A$ be a $2\times 2$ matrix with real entries, and $\lambda = a-bi$ a complex eigenvalue for some eigenvector v. Then what $P$ and $C$ in the equation $A=PCP^{-1}$}
	\xfield{$P = (\text{Re } v\ \text{Im } v)$ invertible and $C=\begin{pmatrix}
	a&-b\\
	b&a
	\end{pmatrix}$ }
\end{note}

\begin{note}
	\xfield{
	For the matrices $u=\begin{pmatrix}
	u_1\\
	u_2\\
	\vdots\\
	u_d
	\end{pmatrix}$ and $v=\begin{pmatrix}
	v_1\\
	v_2\\
	\vdots\\
	v_d
	\end{pmatrix}$
	\begin{enumerate}
	\item Define the inner product of $u$ and $v$ ($u 	\bullet v$)
	\item Define the length of $v$
	\item What it means to "normalize" $v$
	\item What is the distance of $u$ and $v$
	\item What it means if $u$ and $v$ are orthogonal
	\end{enumerate} }
	\xfield{\begin{enumerate}
	\item $u \bullet v = u^T v=u_1 v_1+u_2 v_2+...+u_d v_d $
	\item length of $v$ : $||v|| = \sqrt{v\bullet v} = \sqrt{v_1^2+v_2^2+...+v_d^2}$
	\item $v$ normalized is an unit vector parralel to $v$: $\bar{v} = \frac{1}{\sqrt{v\bullet v}}v$ (notice that $||\bar{v}|| = 1$)
		\item the distance of $u$ and $v$ : $||u-v|| = ||v-u|| = \sqrt{(u-v) \bullet (u-v)} = \sqrt{(v-u)\bullet (v-u)}$
	\end{enumerate} }
\end{note}

%\begin{note}
%	\xfield{\begin{enumerate}
%	\item Define what an orthogonal set is.
%	\item Define what an orthonormal set is.
%	\end{enumerate} }
%	\xfield{\begin{enumerate}
%	
%	\end{enumerate} }
%\end{note}
\end{document}